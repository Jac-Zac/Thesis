{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5947f1-5e6f-4f97-9330-06e3f4dba400",
   "metadata": {},
   "source": [
    "# Notebook to run the model on unseen images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12be7d-a57f-4877-8eab-ee3b2f0e427e",
   "metadata": {},
   "source": [
    "### Install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dac7e13-4e87-4fd7-96e0-d8a5aabf099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets sentencepiece\n",
    "!pip install -q pytorch-lightning wandb\n",
    "!pip install -q donut-python\n",
    "\n",
    "# !huggingface-cli login this shouldh be done from the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e23404-c018-4a47-8c11-5e628f8b7908",
   "metadata": {},
   "source": [
    "## Resize the images\n",
    "> Image 005294.jpg was wierd\n",
    "\n",
    "I want to have the images in the correct size and flip them on the correct side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e99b72-6cca-471e-a858-00d9cba72bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define the paths for the input and output directories\n",
    "input_dir = \"../donut_example/Immagini_Esposito\"\n",
    "output_dir = \"img_resized/\"\n",
    "size = (1600,1200)\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through all the image files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        # Open the image and resize it\n",
    "        with Image.open(os.path.join(input_dir, filename)) as img:\n",
    "            \n",
    "            # Resize the image to a specific size\n",
    "            img = img.resize(size)\n",
    "            \n",
    "            # Automatically rotate the image based on its EXIF orientation metadata\n",
    "            img = ImageOps.exif_transpose(img)\n",
    "            \n",
    "            # Check if the image is in landscape orientation\n",
    "            if img.width > img.height:\n",
    "                print(filename)\n",
    "                \n",
    "                # Rotate the image 90 degrees clockwise\n",
    "                img = img.rotate(-90, expand=True)\n",
    "            \n",
    "            # Save the cropped and resized image to the output directory\n",
    "            img.save(os.path.join(output_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd632a29-c874-4367-8897-f8427d52a9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bcb94e645246d5a8116db4969ada7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)f6f0360f/config.json:   0%|          | 0.00/5.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d680271c0047c3b3bbcf9f0c4d28aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8626900a8d3042448015efbe934bb9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Using the model that I think works the best and generalize which is epoch 9 of the last run (very similar to epoch 10)\n",
    "processor = DonutProcessor.from_pretrained(\"Jac-Zac/thesis_test_donut\",  revision=\"ba396d4b3d39a4eaf7c8d4919b384ebcf6f0360f\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"Jac-Zac/thesis_test_donut\",  revision=\"ba396d4b3d39a4eaf7c8d4919b384ebcf6f0360f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa831e29-7da2-4187-b30c-a071f1578f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51de0daf70e43749199e8a253564cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d466ec6863504c0bad27f0ca4c0c4d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4f14a3b642473a8464b16d20844e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/Users/jaczac/.cache/huggingface/datasets/imagefolder/img_resized-7f5590504a871c24/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec24c3e25bba4b7c959ed578280afb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from donut import JSONParseEvaluator\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "output_list = []\n",
    "accs = []\n",
    "\n",
    "images_path = \"img_resized\"\n",
    "\n",
    "# Loop through all the image files in the input directory\n",
    "for filename in os.listdir(images_path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        # Load the image\n",
    "        image = Image.open(os.path.join(images_path, filename))\n",
    "        # Prepare encoder inputs\n",
    "        pixel_values = processor(image.convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        # prepare decoder inputs\n",
    "        task_prompt = \"<s_herbarium>\"\n",
    "        decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "        decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "        # autoregressively generate sequence\n",
    "        outputs = model.generate(\n",
    "                pixel_values,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                max_length=model.decoder.config.max_position_embeddings,\n",
    "    #            early_stopping=True,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                output_attentions=True, # Returns the attention tensors of all attention layers\n",
    "                num_beams=1,\n",
    "                bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "\n",
    "        # turn into JSON\n",
    "        seq = processor.batch_decode(outputs.sequences)[0]\n",
    "        seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "        seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "        seq = processor.token2json(seq)\n",
    "\n",
    "        output_list.append({\"filename\": filename ,\"prediction\": seq})\n",
    "    \n",
    "# Save output to JSON file\n",
    "output_file_path = \"../output.json\"  # Replace with your desired output file path\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    json.dump(output_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
