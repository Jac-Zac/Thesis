{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac7e13-4e87-4fd7-96e0-d8a5aabf099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets sentencepiece\n",
    "!pip install -q pytorch-lightning wandb\n",
    "!pip install -q donut-python\n",
    "\n",
    "# !huggingface-cli login this shouldh be done from the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd632a29-c874-4367-8897-f8427d52a9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e7b136dbbb4a8a8e32b67b0cbcd59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"Jac-Zac/thesis_test_donut\", revision = '8c5467cb66685e801ec6ff8de7e7fdd247274ed0')\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"Jac-Zac/thesis_test_donut\", revision = '8c5467cb66685e801ec6ff8de7e7fdd247274ed0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa831e29-7da2-4187-b30c-a071f1578f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c60591ec8884441abbf67ea684dc39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e966b0b8887644d2b5326f581ea164d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4ce353a4e2447198b84142556105d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/Users/jaczac/.cache/huggingface/datasets/imagefolder/img_resized-ab7fa470c2235037/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a512b40cea3f4ddfab83b65fdb4f1706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracies': [0.75, 0.6616161616161615, 0.30508474576271183, 0.7517241379310344, 0.8714285714285714, 0.7698412698412699, 0.7395833333333333, 1.0, 0.8105263157894737, 0.6515151515151515, 0.7564102564102564, 0.7133333333333334, 0.607843137254902, 0.22330097087378642, 0, 0.4, 0.688, 0.9166666666666666, 0.5932203389830508, 0.5977011494252873, 0.7564102564102564, 0.6712328767123288, 0.6056338028169015, 0.8961038961038961, 0.4742268041237113, 0.9380530973451328, 0.8407079646017699, 0.16666666666666663, 0.8256880733944953, 0.9893617021276596, 0.8118811881188119, 1.0, 0.9010989010989011, 0.07594936708860756, 0.6867469879518072, 0.6896551724137931, 0.5705882352941176, 0.6210526315789473, 1.0, 0.7636363636363637, 0.8875, 0.8727272727272728, 0.84375, 0.9230769230769231, 0.7560975609756098, 0.9894736842105263, 1.0, 0.8907563025210083, 0.6, 0.8823529411764706, 0.7529411764705882, 0.7710843373493976, 0.9617834394904459, 1.0, 0.868421052631579, 0.9855072463768116, 0.96, 0.6634615384615384, 0.8648648648648649, 0.9325842696629214, 0.8554216867469879, 0.7108433734939759, 1.0, 0.5220588235294117, 1.0, 0.9926470588235294, 1.0, 0, 0.8376623376623377, 0.7037037037037037, 0.954954954954955, 0.9886363636363636, 0.28888888888888886, 0, 0.7669902912621359, 1.0, 0.868421052631579, 0.922077922077922, 0.75, 0.8850574712643678, 1.0, 0.9152542372881356, 0.3529411764705882, 0.9893617021276596, 0.9247311827956989, 1.0, 0.6956521739130435, 0.7669902912621359, 0.987012987012987, 0.971830985915493, 0.9861111111111112, 0.8405797101449275, 0.8, 1.0, 0.90625, 0.9929078014184397, 0.943089430894309, 0.4954128440366973, 0.9663865546218487, 0.8773584905660378, 0.5, 1.0, 0.7222222222222222, 0.9051094890510949, 0.13907284768211925, 0.7482993197278911, 0.9940828402366864, 0.8425925925925926, 0.7634408602150538, 0.8247422680412371, 0.989247311827957, 0.8484848484848485, 0.9072164948453608, 0.7857142857142857, 1.0, 0.8141592920353982, 0.9661016949152542, 0.892156862745098, 0.8350515463917526, 0.8055555555555556, 0.9908256880733946, 0.9819819819819819, 0.17592592592592593, 0.65, 0.7916666666666666, 0.9014084507042254, 0.7142857142857143, 0, 0.875, 0.4, 0.7959183673469388, 0.9381443298969072, 0.8045977011494253, 1.0, 0.7419354838709677, 0.20588235294117652, 0.4328358208955224, 0.7758620689655172], 'mean_accuracy': 0.7638958523977475} length : 138\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from donut import JSONParseEvaluator\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "output_list = []\n",
    "accs = []\n",
    "\n",
    "image_path = \"/Users/jaczac/Github/Thesis/donut_example/copy/img_resized\"\n",
    "\n",
    "dataset = load_dataset(image_path, split=\"test\")\n",
    "\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # prepare decoder inputs\n",
    "    task_prompt = \"<s_herbarium>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    \n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "    \n",
    "    # modify ground_truth to replace \" \" with \"\" since I would still count it as a correct prediction\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"].replace('\" \"', '\"\"'))\n",
    "    \n",
    "    evaluator = JSONParseEvaluator()\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "\n",
    "    accs.append(score)\n",
    "    output_list.append(seq)\n",
    "\n",
    "scores = {\"accuracies\": accs, \"mean_accuracy\": np.mean(accs)}\n",
    "print(scores, f\"length : {len(accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ce695c8-2ecb-4d82-9253-e8feb127aed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7638958523977475\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean accuracy:\", np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "89eca271-9024-4a77-b9ad-9809212f94fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.830369809893124\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean accuracy:\", np.median(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bddc6df2-d0f4-42fc-9a8e-87a9443aea20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy (excluding worst 10): 0.8158658554664913\n"
     ]
    }
   ],
   "source": [
    "mean_without_worst = np.mean(np.sort(accs)[10:])\n",
    "print(\"Mean accuracy (excluding worst 10):\", mean_without_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2822bb-75f7-4615-abda-f868461c5ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get indices of worst 5 predictions\n",
    "worst_idxs = np.argsort(accs)[:10].tolist()\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_herbarium>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "for idx in worst_idxs:\n",
    "    sample = dataset[idx]\n",
    "\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "    \n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\\n\")\n",
    "    print(f\"Prediction: {seq}\\n\")\n",
    "    print(f\"Score: {accs[idx]}\\n\")\n",
    "    display(sample[\"image\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
