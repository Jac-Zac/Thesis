{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c88f97c-404c-4ba9-bc80-0c2bc6e4f2ea",
    "_uuid": "67a1c654-2385-4ede-a77c-c39926ddf9e0",
    "id": "DNMqJ821yNVo"
   },
   "source": [
    "# Document Understanding Transformer (Donut)\n",
    "> quick intro by ClovaAI\n",
    "\n",
    "Document Understanding Transformer (Donut) is a new Transformer model for OCR-free document understanding. It doesn't require an OCR engine to process scanned documents but is achieving state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing). \n",
    "Donut is a multimodal sequence-to-sequence model with a vision encoder ([Swin Transformer](https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/swin#overview)) and text decoder ([BART](https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/bart)). The encoder receives the images and computes it into an embedding, which is then passed to the decoder, which generates a sequence of tokens.\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2111.15664\n",
    "* Official repo:  https://github.com/clovaai/donut\n",
    "\n",
    "---\n",
    "\n",
    "### Main step:\n",
    "\n",
    "- Load the custom dataset and make it in the rigth format\n",
    "- Load the model and processor\n",
    "- Create pytorch dataset\n",
    "- Create PyTorch DataLoader\n",
    "- Define LightningModule\n",
    "- Training\n",
    "- Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6d7ec66-f70e-4ff3-8a11-6b07d680c3ee",
    "_uuid": "df740718-bf3f-4ce5-a12a-75017faea521",
    "id": "_we6V91TRW-K"
   },
   "source": [
    "#### Starting point\n",
    "\n",
    "First, let's install the relevant libraries:\n",
    "* ðŸ¤— Transformers, for the model\n",
    "* ðŸ¤— Datasets, for loading + processing the data\n",
    "* PyTorch Lightning, for training the model \n",
    "* Weights and Biases, for logging metrics during training\n",
    "* Sentencepiece, used for tokenization.\n",
    "\n",
    "We'll use PyTorch Lightning for training here, but note that this is optional, you can of course also just train in native PyTorch or use ðŸ¤— Accelerate, or the ðŸ¤— Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea73dcb8-f5ce-487b-9171-c0814647ef0e",
    "_uuid": "147c58c8-34a6-433b-aaaf-5eeaa31e0117",
    "collapsed": false,
    "id": "ot1nP9YHz8co",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "cbaaf9e2-7c3a-4d57-8ee6-3820a59ddc91"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers sentencepiece\n",
    "!pip install -q --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7c736e1d-c35d-452f-ad8d-0a9a15d21a8e",
    "_uuid": "079aab70-be8f-423e-8450-7fd138a0dbf2",
    "collapsed": false,
    "id": "zMZ6tiMB1JxD",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "edb59e50-a618-4e9a-c161-35a0563c1f04"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning>=1.1.8\n",
    "!pip install -q \"wandb>=0.15.8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "75576603-0965-47d7-803f-968704f367f9",
    "_uuid": "c55763c8-0b9c-4627-83cd-591376054c7e",
    "id": "kWYic8VNyDNU"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "We will load a version of the custom dataset from my google drive. To the google collab. And then get in the correct directory to then work with it. Also do some cleanups of useless files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8ba4e138-eac8-4281-9066-0fe2c2fc6834",
    "_uuid": "cb8c0508-00d5-4b05-b942-6c257c5de104",
    "id": "pfjVjWssvTOM"
   },
   "source": [
    "## 3. Creating dataset\n",
    "\n",
    "> This is a good starting point if your dataset is already formatted correctly\n",
    "\n",
    "Since our custom dataset is not compatible with Donut, we will use the `imagefolder` feature of the `datasets` library to load the dataset into our model. \n",
    "Now we can load the dataset using the `imagefolder` feature of `datasets`.\n",
    "___\n",
    "\n",
    "#### Notes\n",
    "This might be interesting [sparrow](https://github.com/katanaml/sparrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d2dbbc78-6d73-4cef-ad2b-ace3661daee5",
    "_uuid": "129fcae6-a15c-45f6-b0db-a988ec47c021",
    "collapsed": false,
    "id": "19_16z8-vaQ7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Delete in case of cached dataset\n",
    "!rm -rf /root/.cache/huggingface/datasets/imagefolder/img_resized-6cad3f41b5b1940e/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff\n",
    "%cd /kaggle/input/thesis-resized-full-data\n",
    "\n",
    "# !mv img_resized/val img_resized/validation # might be the reason why it is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8943faaf-1f98-4f85-8c04-f645ac343570",
    "_uuid": "3b9f9c7f-4625-43ae-8401-a98e80d08789",
    "collapsed": false,
    "id": "H2NZiA5XvXdR",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f2c20f99-38a9-4e5b-ae7c-ece3eafe1df9"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# image_path = base_path.joinpath(\"img\")\n",
    "image_path = \"img_resized\"\n",
    "\n",
    "dataset = load_dataset(image_path)\n",
    "\n",
    "print(f'this is the dataset {dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f7eccd4-7885-47df-879f-c64c5bec027a",
    "_uuid": "957a85cd-bfbc-475c-a6b3-b1f60d05dc91",
    "id": "-kFsrrh3jObj"
   },
   "source": [
    "#### Show an example\n",
    "\n",
    "Now, lets take a closer look at our dataset by showing an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc3434ad-f649-4fee-893d-54049bce2ebc",
    "_uuid": "15254abd-8709-40fb-9619-f258b55aac3a",
    "collapsed": false,
    "id": "nHVKQMQvy_uU",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "97a1c10b-6190-4a6a-fe5c-5dacf3850639"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_sample = random.randint(0, len(dataset['train']))\n",
    "\n",
    "example = dataset['train'][random_sample]\n",
    "image = example['image']\n",
    "ground_truth = example['ground_truth']\n",
    " \n",
    "# Print the nmae of the sample\n",
    "print(f\"Random sample is {random_sample}\")\n",
    "        \n",
    "# let's load the corresponding JSON dictionary (as string representation)\n",
    "print(f\"OCR text is {ground_truth}\")\n",
    "\n",
    "# let's make the image a bit smaller when visualizing\n",
    "width, height = image.size\n",
    "display(image.resize((int(width*0.3), int(height*0.3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "872a7c6e-fc4c-4766-a6c4-66715f3cb05e",
    "_uuid": "0624ffbe-7d79-43bd-a7f2-e55d2d498e8c",
    "id": "8V8Rgq4jwoqL"
   },
   "source": [
    "We can also parse the string as a Python dictionary using `ast.literal_eval`. Each training example has a single \"gt_parse\" key, which contains the ground truth parsing of the document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e7641a8-f2c1-4cc6-a02f-ce96d135ecb7",
    "_uuid": "b9d32637-0eb2-47fc-9eca-acef45b99148",
    "id": "BCjMK93Cz3zf"
   },
   "source": [
    "## Load model and processor\n",
    "\n",
    "Next, we load the model (Donut is an instance of [VisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder)), and the processor, which is the object that can be used to prepare inputs for the model.\n",
    "\n",
    "We'll update some settings for fine-tuning, namely the image size and the max length of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "423c668c-728b-454b-8073-327e901e91d8",
    "_uuid": "a7711b4f-8a16-4e3d-b073-3bbb9dd4536a",
    "collapsed": false,
    "id": "ahkkeo8_o69z",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9af18b96-396a-4921-c28d-ef3e0c94b8a1"
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "image_size = [1600, 1200]\n",
    "# image_size = [1200, 1600] # alternative\n",
    "max_length = 768 # used to be 1024 for the one that were not working\n",
    "\n",
    "# update image_size of the encoder\n",
    "# during pre-training, a larger image size was used\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "config.encoder.image_size = image_size # (height, width)\n",
    "# update max_length of the decoder (for generation)\n",
    "config.decoder.max_length = max_length\n",
    "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
    "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28421113-41f6-4270-9bc4-652a745f4eb4",
    "_uuid": "d2d79e1f-659c-4b9b-81ec-5310af8015c4",
    "id": "MUiQda9_mjAC"
   },
   "source": [
    "Next, we instantiate the model with our custom config, as well as the processor. Make sure that all pre-trained weights are correctly loaded (a warning would tell you if that's not the case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fe9b1ef6-3670-4836-b3d7-2031dbfd5ac2",
    "_uuid": "6cdd6bd7-6cf4-4a78-97f5-4e8f26a54341",
    "collapsed": false,
    "id": "84TkZP5zz4hE",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "78d175af-3b18-40fa-d2a9-a2fa81e1f0aa"
   },
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "547b286d-d5fd-4d8d-88ad-13e74e8bdebc",
    "_uuid": "1d099cf0-e2d4-4ae0-99f9-07a50bd9e55a",
    "id": "b46s3KR-x8Iv"
   },
   "source": [
    "## Create PyTorch dataset\n",
    "\n",
    "Here we create a regular PyTorch dataset.\n",
    "\n",
    "The model doesn't directly take the (image, JSON) pairs as input and labels. Rather, we create `pixel_values` and `labels`. Both are PyTorch tensors. The `pixel_values` are the input images (resized, padded and normalized), and the `labels` are the `input_ids` of the target sequence (which is a flattened version of the JSON), with padding tokens replaced by -100 (to make sure these are ignored by the loss function). Both are created using `DonutProcessor` (which internally combines an image processor, for the image modality, and a tokenizer, for the text modality).\n",
    "\n",
    "Note that we're also adding tokens to the vocabulary of the decoder (and corresponding tokenizer) for all keys of the dictionaries in our dataset, like \"\\<s_menu>\". This makes sure the model learns an embedding vector for them. Without doing this, some keys might get split up into multiple subword tokens, in which case the model just learns an embedding for the subword tokens, rather than a direct embedding for these keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d961c828-8105-4a0a-a5de-bb91718a73b9",
    "_uuid": "95fa5a98-f835-4492-9b09-4597784a1d9f",
    "collapsed": false,
    "id": "7tWX_qJDvw_S",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class DonutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Donut. This class takes a HuggingFace Dataset as input.\n",
    "    \n",
    "    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n",
    "    and it will be converted into pixel_values (vectorized image) and labels (input_ids of the tokenized string).\n",
    "    \n",
    "    Args:\n",
    "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
    "        max_length: the max number of tokens for the target sequences\n",
    "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
    "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
    "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
    "        prompt_end_token: the special token at the end of the sequences\n",
    "        sort_json_key: whether or not to sort the JSON keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        max_length: int,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        task_start_token: str = \"<s>\",\n",
    "        prompt_end_token: str = None,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.task_start_token = task_start_token\n",
    "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if 'gt_parse' not in ground_truth and 'gt_parses' not in ground_truth:\n",
    "                # If the ground truth is already a string then just save that\n",
    "                gt_jsons = [ground_truth]\n",
    "            else:\n",
    "                if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
    "                    assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                    gt_jsons = ground_truth[\"gt_parses\"]\n",
    "                else:\n",
    "                    assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                    gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            # Add the tokens indipentently of how they are created\n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        gt_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    + processor.tokenizer.eos_token\n",
    "                    for gt_json in gt_jsons  # load json from list of json\n",
    "                ]\n",
    "             )\n",
    "\n",
    "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
    "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "    \n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
    "        Convert gt data into input_ids (tokenized string)\n",
    "        Returns:\n",
    "            input_tensor : preprocessed image\n",
    "            input_ids : tokenized gt_data\n",
    "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        pixel_values = processor(sample[\"image\"], random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.squeeze()\n",
    "\n",
    "        # targets\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n",
    "        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
    "        return pixel_values, labels, target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35218d17-310e-4f1c-9394-7ff4c17ef02e",
    "_uuid": "754dcd98-97b2-420a-b98e-7e7ef349c5b2",
    "id": "KBseZac0m_W8"
   },
   "source": [
    "Next, we instantiate the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aa64cfe5-1dc8-4189-9c7a-12c79713b204",
    "_uuid": "193d4c22-c5eb-4a70-b680-404076cb655e",
    "collapsed": false,
    "id": "3JpazNkf8CnA",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "659b14d2-2b6d-4292-fec7-817a00f4eec2"
   },
   "outputs": [],
   "source": [
    "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
    "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
    "processor.image_processor.size = image_size[::-1] # should be (width, height)\n",
    "processor.image_processor.do_align_long_axis = False\n",
    "\n",
    "train_dataset = DonutDataset(image_path, max_length=max_length,\n",
    "                             split=\"train\", task_start_token=\"<s_herbarium>\", prompt_end_token=\"<s_herbarium>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )\n",
    "\n",
    "val_dataset = DonutDataset(image_path, max_length=max_length,\n",
    "                             split=\"validation\", task_start_token=\"<s_herbarium>\", prompt_end_token=\"<s_herbarium>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8602d0fd-a2fc-4513-a1bd-227de4a5987e",
    "_uuid": "b89866df-2c96-464b-9c25-ce90ef16f09a",
    "id": "QWYmtYxIoHnX"
   },
   "source": [
    "Let's check which tokens are added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dd6cc095-af75-4e4c-8dc7-a694b254d111",
    "_uuid": "2291fdb4-9e55-4aed-afb8-d624db709047",
    "collapsed": false,
    "id": "W0Z9JhK3E7WR",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "67719148-9ca2-439a-9933-9e5d1e49177e"
   },
   "outputs": [],
   "source": [
    "# the vocab size attribute stays constants (might be a bit unintuitive - but doesn't include special tokens)\n",
    "print(\"Original number of tokens:\", processor.tokenizer.vocab_size)\n",
    "print(\"Number of tokens after adding special tokens:\", len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2cb645c-f285-482f-b9dd-4999c9741a63",
    "_uuid": "6eb926a9-5925-4e04-91cf-866788ce1fe7",
    "id": "bd5mNnuPqUAN"
   },
   "source": [
    "As always, it's very important to verify whether our data is prepared correctly. Let's check the first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "50d1aa7a-7f7e-40b3-99f2-f22f28f8ad63",
    "_uuid": "2102ee08-489f-4c71-9283-91e8ba3f7a50",
    "collapsed": false,
    "id": "mkHzamYl90we",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pixel_values, labels, target_sequence = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00dd599a-3f91-4b36-b510-00a3d5474136",
    "_uuid": "c8089b7f-040f-4e81-98f8-9ae865af3589",
    "id": "07bHWGlFtpIg"
   },
   "source": [
    "This returns the `pixel_values` (the image, but prepared for the model as a PyTorch tensor), the `labels` (which are the encoded `input_ids` of the target sequence, which we want Donut to learn to generate) and the original `target_sequence`. The reason we also return the latter is because this will allow us to compute metrics between the generated sequences and the ground truth target sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d73050d-f537-4a49-a490-9e6ed02561ac",
    "_uuid": "6748ae07-8882-49cb-81c9-e5a96f87363f",
    "collapsed": false,
    "id": "kNyN_Af0-QMA",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "c1f6c87e-7d86-4147-c183-ef9fa18ffcc8"
   },
   "outputs": [],
   "source": [
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f154aaf0-036c-4743-87d2-7cef0a280f3d",
    "_uuid": "83592ea1-04b6-49c6-bc1d-b16c4805efbd",
    "id": "poYwvFdAdikM"
   },
   "source": [
    "Another important thing is that we need to set 2 additional attributes in the configuration of the model. This is not required, but will allow us to train the model by only providing the decoder targets, without having to provide any decoder inputs.\n",
    "\n",
    "The model will automatically create the `decoder_input_ids` (the decoder inputs) based on the `labels`, by shifting them one position to the right and prepending the decoder_start_token_id. I recommend checking [this video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&t=888s&ab_channel=NielsRogge) if you want to understand how models like Donut automatically create decoder_input_ids - and more broadly how Donut works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d3efe2b7-cb5f-4009-991d-24c1644a1239",
    "_uuid": "47caa283-e71e-4ce9-8dc0-abb319589d8c",
    "collapsed": false,
    "id": "VgRLEVMdc1lx",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_herbarium>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c2e72888-a835-4110-8722-9d22a9b20e08",
    "_uuid": "0a458adb-e800-40e1-ae77-87d4d48a64c3",
    "collapsed": false,
    "id": "bTn0nmssdRWE",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b90e2f98-8118-4af7-ed27-9674235c4ae3"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
    "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da7f7d03-3a54-4697-9ff8-eb4dbe7966c3",
    "_uuid": "1028d5bc-276c-4992-9a5a-7e8e088f885d",
    "id": "ygTIylugfasG"
   },
   "source": [
    "## Create PyTorch DataLoaders\n",
    "\n",
    "Next, we create corresponding PyTorch DataLoaders, which allow us to loop over the dataset in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "394f3315-5f50-4b26-8ace-cd5eb59bfc1f",
    "_uuid": "d24a9038-f098-4863-8319-38f6e368f900",
    "collapsed": false,
    "id": "nLQ_Vl5MLugu",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# feel free to increase the batch size if you have a lot of memory\n",
    "# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "#train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3e888dd-124a-4500-8158-750d418a10ec",
    "_uuid": "004fd49f-6e1b-49f2-8c0c-20addd747a5f",
    "id": "AxtTVgNnfdkD"
   },
   "source": [
    "Let's verify a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5227cdd-d12e-4714-ba1c-f278b94499f6",
    "_uuid": "04e79800-90d1-4a2f-9229-427bfbd2f402",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30721012-f67f-4c18-8919-3306681f9e5a",
    "_uuid": "dd150d51-2b20-49a8-abf6-4c4a92d4bf06",
    "collapsed": false,
    "id": "WHurHlLnL8Xm",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "629fa3dc-f8a1-4f23-fcbb-78b273a98bfa"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d65bf7bd-e668-4566-bda2-be2bd8fc4e8d",
    "_uuid": "07e3f223-6a9f-4652-90c9-2f2d567fbec2",
    "collapsed": false,
    "id": "xbCw5mYH0Mvu",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7fb51cb8-78a9-4b1f-f989-c9529e6dbcf8"
   },
   "outputs": [],
   "source": [
    "print(target_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4cfa5317-6982-4080-843a-02a38a0d9aff",
    "_uuid": "c4432cf3-0bb4-4b89-932e-2cb4335fed82",
    "id": "mnmD7rRy2WLI"
   },
   "source": [
    "## Define LightningModule\n",
    "\n",
    "Next, we define a [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html), which is the standard way to train a model in PyTorch Lightning. A LightningModule is an `nn.Module` with some additional functionality. \n",
    "\n",
    "Basically, PyTorch Lightning will take care of all device placements (`.to(device)`) for us, as well as the backward pass, putting the model in training mode, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc9e5b77-0bb2-48d0-b1eb-15a4a4308fc1",
    "_uuid": "3d9e0ad6-ee79-493b-a1f9-a13d93cb994d",
    "collapsed": false,
    "id": "oRm5i4gWG-sb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "class DonutModelPLModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        dropout_rate = config['dropout_rate']  # Access the dropout_rate from the config\n",
    "        model.config.hidden_dropout_prob = dropout_rate\n",
    "        model.config.attention_probs_dropout_prob = dropout_rate\n",
    "        self.model = model\n",
    "        # save hyperparameters\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, labels, _ = batch\n",
    "        \n",
    "        outputs = self.model(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        pixel_values, labels, answers = batch\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        # we feed the prompt to the model\n",
    "        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n",
    "        \n",
    "        outputs = self.model.generate(pixel_values,\n",
    "                                   decoder_input_ids=decoder_input_ids,\n",
    "                                   max_length=max_length,\n",
    "                                   early_stopping=False,\n",
    "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                                   use_cache=True,\n",
    "                                   num_beams=1,\n",
    "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                                   return_dict_in_generate=True,)\n",
    "    \n",
    "        predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "            predictions.append(seq)\n",
    "\n",
    "        scores = []\n",
    "        accuracies = []  # Track validation accuracies\n",
    "        \n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            # NOT NEEDED ANYMORE\n",
    "            # answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
    "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "            \n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                pass\n",
    "                # print(f\"Prediction: {pred}\")\n",
    "                # print(f\"    Answer: {answer}\")\n",
    "                # print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "            # Calculate accuracy and append to accuracies list\n",
    "            accuracy = 1 - scores[-1]  # Subtract edit distance from 1 to get accuracy\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "        self.log(\"val_accuracy\", np.mean(accuracies))  # Log the mean validation accuracy\n",
    "\n",
    "        # Log examples with 0 accuracy\n",
    "        if score == 0:\n",
    "\n",
    "            # Create a wandb.Table with the appropriate columns\n",
    "            table = wandb.Table(columns=[\"Image\", \"Prediction\", \"Ground Truth\"])\n",
    "            \n",
    "            for pixel_value, pred, answer, score in zip(pixel_values, predictions, answers, scores):\n",
    "                    # Convert image to PIL Image\n",
    "                    image = ToPILImage()(pixel_value.cpu().numpy())\n",
    "        \n",
    "                    # Resize image to 1200x900\n",
    "                    image = image.resize((1200, 900))\n",
    "        \n",
    "                    # Convert image back to numpy array\n",
    "                    image = np.array(image)\n",
    "        \n",
    "                    # Add data to the table\n",
    "                    table.add_data(image, pred, answer)\n",
    "        \n",
    "            # Log the table to Weights & Biases\n",
    "            wandb.log({\"val_zero_score_examples\": table})\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "    \n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "660d7294-cbb6-470f-9926-cbdb80d5117f",
    "_uuid": "fc33c492-f907-4575-8a25-9e5fbe2c917c",
    "id": "0ZoPiDOPKg0o"
   },
   "source": [
    "## Train!\n",
    "\n",
    "Next, let's train! This happens instantiating a PyTorch Lightning `Trainer`, and then calling `trainer.fit`.\n",
    "\n",
    "What's great is that we can automatically train on the hardware we have (in our case, a single GPU), enable mixed precision (`fp16=True`, which makes sure we don't consume as much memory), add Weights and Biases logging, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6e5def15-4c47-4f73-8509-55a9d9b0fc82",
    "_uuid": "9f168d6d-df01-4e9e-bff1-47c3572bca48",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"/kaggle/working/output\"\n",
    "result_dir = os.path.join(output_dir, \"result\")\n",
    "\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "%cd \"/kaggle/working/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /kaggle/working/output/Donut/version_None/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "783bc15a-023d-4c51-be55-7a97043c056f",
    "_uuid": "0dad5ccc-0aba-4233-bb8f-d7d9670339f5",
    "collapsed": false,
    "id": "pxNJhCGjKhtR",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_epochs': 20,\n",
    "    'val_check_interval': 0.25,\n",
    "    'check_val_every_n_epoch': 1,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    'num_training_samples_per_epoch': 36760,\n",
    "    'lr': 2e-5, # or 2e-5\n",
    "    'weight_decay': 2e-5,\n",
    "    'dropout_rate': 0.15,\n",
    "    'train_batch_sizes': [8],\n",
    "    'val_batch_sizes': [1],\n",
    "    'num_nodes': 1,\n",
    "    'warmup_steps': 2500,\n",
    "    'result_path': \"/kaggle/working/output/result\",\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "model_module = DonutModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c71e04c0-8887-48dd-abdf-5c58d4bad6b6",
    "_uuid": "c6b724a4-20ca-42e6-a703-81b9a69d66b5",
    "id": "6l4byTwPRBZx"
   },
   "source": [
    "We'll use a custom callback to push our model to the hub during training (after each epoch + end of training). For that we'll log into our HuggingFace account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c5ccd86-8957-4b1d-8d09-6fda0b05db82",
    "_uuid": "8386b9d9-6466-45f4-a04c-6cbe97c7f82b",
    "collapsed": false,
    "id": "7C9RWsZrQlEg",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "617ded44-8cd2-4920-aec0-71ea9cc4a17d"
   },
   "outputs": [],
   "source": [
    "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('hugginface_token')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6cceff13-6c93-4548-92ac-bd524cff2252",
    "_uuid": "b6be4363-3350-4788-8f34-3f983572f418",
    "collapsed": false,
    "id": "NiK6-vQHKnBy",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "\n",
    "# api key so that it doesn't ask me for it\n",
    "api_key = \"api_key\"\n",
    "wandb.login(key=api_key)\n",
    "wandb_logger = WandbLogger(project=\"Donut\", name=\"thesis_donut\")\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        # pl_module.processor.push_to_hub(\"Jac-Zac/thesis_donut\",\n",
    "        #                           commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(\"Jac-Zac/thesis_donut\",\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(\"Jac-Zac/thesis_donut\",\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(\"Jac-Zac/thesis_donut\",\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", verbose=True, mode=\"min\") # use default patiente\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        #accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        # strategy=\"xla_debug\",\n",
    "        max_epochs=config['max_epochs'],\n",
    "        val_check_interval=config['val_check_interval'],\n",
    "        check_val_every_n_epoch=config['check_val_every_n_epoch'],\n",
    "        gradient_clip_val=config['gradient_clip_val'],\n",
    "        precision=\"16-mixed\", # we'll use mixed precision\n",
    "        #precision=16, # we'll use mixed precision\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[PushToHubCallback(), early_stop_callback],\n",
    ")\n",
    "\n",
    "# trainer.fit(model_module, ckpt_path = '/kaggle/working/output/Donut/version_None/checkpoints/epoch=0-step=32164.ckpt')\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1de3d93-c877-4c59-b25d-fb5622c7fb36",
    "_uuid": "7c21b7c8-c855-41cc-8dee-9d9f7a03b81c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !rm -r /kaggle/working/output/Donut\n",
    "# !pip install -U torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a7bb3e2-b376-4b1a-9673-2b4df3eb444b",
    "_uuid": "ad45caef-5ba0-4cda-824d-324a537c6fe8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print(decoder_input_ids.max())\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018dba43-d9b3-41ad-9b87-3378b13d53f3",
    "_uuid": "aa2249ce-ad3f-447d-8f60-7f9cb479985c"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "After training, we can evaluate the model on the test set.\n",
    "\n",
    "As we pushed the model to the hub, we can very easily load it back again using the `from_pretrained` method. You can see the files in this [repo](https://huggingface.co/Jac-Zac/thesis_test_donut)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6dfc1ce0-ef81-4415-9a35-fe2b9cc87501",
    "_uuid": "3e9949bb-5b15-4b6d-a1d9-550932ab617a"
   },
   "source": [
    "Note that you can also easily refer to a specific commit in the `from_pretrained` method using the [`revision`](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained.revision) argument, or use the private hub in case you'd like to keep your models private and only shared with certain colleagues for instance.\n",
    "\n",
    "Here we're just loading from the main branch, which means the latest commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers sentencepiece\n",
    "!pip install -q --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d973e69-36ce-4737-88af-8ef7ff112b07",
    "_uuid": "bcc57dd9-ba03-4b4f-9f0b-0a3931da3c15",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q --upgrade pytorch-lightning\n",
    "%pip install -q pytorch-lightning wandb\n",
    "%pip install -q donut-python\n",
    "\n",
    "%cd /kaggle/input/thesis-resized-full-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a6c082de-d993-41d7-bd5e-af86712d9486",
    "_uuid": "88a5839a-c452-49da-b647-d665d776fd9b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"Jac-Zac/thesis_donut\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"Jac-Zac/thesis_donut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a02ae329-18ce-4c3b-a144-a5acaa4b64df",
    "_uuid": "9b4f569f-2d56-4331-b0d4-64497e9d8bc9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from donut import JSONParseEvaluator\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "\n",
    "# device = xm.xla_device()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "table = wandb.Table(columns=[\"Image\", \"Prediction\", \"Ground Truth\"])\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "output_list = []\n",
    "accs = []\n",
    "\n",
    "image_path = \"img_resized\"\n",
    "\n",
    "dataset = load_dataset(image_path, split=\"test\")\n",
    "\n",
    "api_key = \"api-key\"\n",
    "wandb.login(key=api_key)\n",
    "wandb.init(project=\"Donut\", name=\"test_set\")\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # prepare decoder inputs\n",
    "    task_prompt = \"<s_herbarium>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "        \n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "    \n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "    evaluator = JSONParseEvaluator()\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "\n",
    "    accs.append(score)\n",
    "    output_list.append(seq)\n",
    "\n",
    "    # upload images with low score to wb\n",
    "    if score == 0:\n",
    "        image = sample[\"image\"].convert(\"RGB\").resize((1200, 900))\n",
    "        pred = seq\n",
    "        gt = sample[\"ground_truth\"]\n",
    "\n",
    "        table.add_data(image, pred, gt)\n",
    "        \n",
    "wandb.log({\"worst_predictions\": table})\n",
    "\n",
    "scores = {\"accuracies\": accs, \"mean_accuracy\": np.mean(accs)}\n",
    "print(scores, f\"length : {len(accs)}\")\n",
    "print(\"Median accuracy:\", np.median(accs))\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8277ad84-03f8-4b90-9284-d97679e8e84a",
    "_uuid": "eabb1114-f4a0-44d1-a5b4-59018c10e695"
   },
   "source": [
    "### What if we do not consider the worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60b9e4a2-c7d8-4435-a86b-b9b4d450f3b4",
    "_uuid": "8d560143-c340-4ffa-bd0b-bca68804e24d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mean_without_worst = np.mean(np.sort(accs)[10:])\n",
    "print(\"Mean accuracy (excluding worst 10):\", mean_without_worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "589a274a-9ba8-446a-aadc-84228b85bb0d",
    "_uuid": "f8b1b1ae-64c0-45c2-98f6-0c42865efbb8"
   },
   "source": [
    "### Example on which the model didn't performe well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4be4100e-4cf3-4551-b8df-2014f5e51f16",
    "_uuid": "6413aa96-2d43-4126-9656-02cc30303968",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get indices of worst 10 predictions\n",
    "worst_idxs = np.argsort(accs)[:100].tolist()\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_herbarium>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "for idx in worst_idxs:\n",
    "    sample = dataset[idx]\n",
    "\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "    \n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\\n\")\n",
    "    print(f\"Prediction: {seq}\\n\")\n",
    "    print(f\"Score: {accs[idx]}\\n\")\n",
    "    display(sample[\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "38fbf7a3-c1cc-4041-8b29-c3f0160bbbfe",
    "_uuid": "2896de25-cfaa-4cf4-a9c0-95e8e8d22ff4"
   },
   "source": [
    "# Download checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d140f1e0-9a4a-4461-97a5-8a8d450415cc",
    "_uuid": "854cafaf-2184-494d-9062-becd8cb1d6ea",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "source_path = '/kaggle/working/output/Donut/version_None/checkpoints/epoch=0-step=19299-v1.ckpt'\n",
    "destination_path = '/kaggle/working/epoch=0-step=19299-v1.ckpt'\n",
    "shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2dcd86af-373f-48b7-af76-60e4817ac718",
    "_uuid": "fb7f7dbf-22bf-4bc3-8fe9-407f126443c0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'epoch=0-step=19299-v1.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
